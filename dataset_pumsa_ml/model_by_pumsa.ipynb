{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "import pandas as pd\n",
    "from konlpy.tag import Okt  # Okt(Open Korean Text) 클래스\n",
    "import nltk  # 자연어 처리 패키지 문서탐색용, Test 클래스\n",
    "import numpy as np  # 행렬, 대규모 다차원 배열을 쉽게 처리 할 수 있도록 지원하는 파이썬의 라이브러리\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras import losses\n",
    "from tensorflow.keras import metrics\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_csv_file(csv_file_name):\n",
    "    \"\"\"csv파일을 dataframe 형식으로 가져오기\"\"\"\n",
    "\n",
    "    df = pd.read_csv(\"./\" + csv_file_name + \".csv\")\n",
    "    return df\n",
    "\n",
    "\n",
    "def tokenize(sentence):\n",
    "    \"\"\"품사\"\"\"\n",
    "\n",
    "    okt = Okt()\n",
    "    return okt.pos(sentence, norm=True, stem=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_non_meaning_pumsa(token_label_sentences):\n",
    "    \"\"\"의미없는 품사 제거\"\"\"\n",
    "\n",
    "    deleted_pumsa_data = []\n",
    "    pumsa_list = [\"Adjective\", \"Adverb\", \"Alpha\", \"Determiner\", \"Exclamation\",\n",
    "                  \"KoreanParticle\", \"Noun\", \"Verb\"]\n",
    "\n",
    "    # token_label_sentences ex)[([('마녀', 'Noun'), ('같다', 'Adjective')], '0')]\n",
    "\n",
    "    for row in token_label_sentences:\n",
    "        for token_word in row[0]:\n",
    "            if token_word[1] in pumsa_list:\n",
    "                deleted_pumsa_data.append([token_word[0], token_word[1], row[1]])\n",
    "    return deleted_pumsa_data\n",
    "\n",
    "\n",
    "def delete_stop_words(token_label_words):\n",
    "    \"\"\"의미없는 단어(불용어) 제거\"\"\"\n",
    "\n",
    "    deleted_stop_words = []\n",
    "    stop_words_list = []\n",
    "\n",
    "    stop_words = read_csv_file(\"stop_words\")\n",
    "    stop_words_list = stop_words['stopword'].tolist()\n",
    "    for row in token_label_words:\n",
    "        if row[0] not in stop_words_list:\n",
    "            deleted_stop_words.append(row)\n",
    "    return deleted_stop_words\n",
    "\n",
    "\n",
    "def remain_meaning_token(token_data):\n",
    "    \"\"\"토큰 전처리\"\"\"\n",
    "\n",
    "    deleted_pumsa_data = delete_non_meaning_pumsa(token_data)\n",
    "    meaning_tokens = delete_stop_words(deleted_pumsa_data)\n",
    "    return meaning_tokens\n",
    "\n",
    "\n",
    "def count_word_frequency(token_sentence, selected_words):\n",
    "    \"\"\"단어 빈도수 측정\"\"\"\n",
    "\n",
    "    token_words = []\n",
    "    for token_word in token_sentence:\n",
    "        token_words.append(token_word[0])  # 단어+품사에서 단어만 추가\n",
    "    return [token_words.count(word) for word in selected_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reindex(data):\n",
    "    \"\"\"인덱스 재정렬\"\"\"\n",
    "\n",
    "    data.reset_index(drop=True)\n",
    "    return data\n",
    "\n",
    "\n",
    "def devide_train_test(comments):\n",
    "    \"\"\"test/train data 분리\"\"\"\n",
    "\n",
    "    train_data = comments.sample(frac=0.7, random_state=2019)  # 7:3 비율로 train, test data 분리\n",
    "    test_data = comments.drop(train_data.index)\n",
    "    reindex(train_data)\n",
    "    reindex(test_data)\n",
    "    return train_data, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_preprocessing(data_file):\n",
    "    \"\"\"데이터 전처리\"\"\"\n",
    "\n",
    "    comments = read_csv_file(data_file)\n",
    "    train_data, test_data = devide_train_test(comments)\n",
    "\n",
    "    train_token_data = [(tokenize(train_data['comment'][ind]), train_data['labeling'][ind]) for ind in\n",
    "                        train_data.index]  # data 토큰화, 단어+품사+라벨\n",
    "    test_token_data = [(tokenize(test_data['comment'][ind]), test_data['labeling'][ind]) for ind in test_data.index]\n",
    "\n",
    "    meaning_tokens = remain_meaning_token(train_token_data)  # 무의미 품사, 불용어 제거, 단어+품사+라벨\n",
    "    tokens = [meaning_token[0] for meaning_token in meaning_tokens]  # 단어\n",
    "    text = nltk.Text(tokens, name='NMSC')\n",
    "    selected_tokens = [common_word[0] for common_word in text.vocab().most_common(100)]  # 자주쓰이는 단어\n",
    "\n",
    "    f=open(\"./commonwords.pkl\", \"wb\")\n",
    "    pickle.dump(selected_tokens, f)\n",
    "    f.close()\n",
    "\n",
    "    train_x = [count_word_frequency(train_token_row[0], selected_tokens) for train_token_row in\n",
    "               train_token_data]  # x : 단어 빈도수 벡터화 y : 0,1,2 라벨\n",
    "    test_x = [count_word_frequency(test_token_row[0], selected_tokens) for test_token_row in test_token_data]\n",
    "    train_y = [train_token_row[1] for train_token_row in train_token_data]\n",
    "    test_y = [test_token_row[1] for test_token_row in test_token_data]\n",
    "\n",
    "    x_train = np.asarray(train_x).astype('float32')\n",
    "    x_test = np.asarray(test_x).astype('float32')\n",
    "    y_train = np.asarray(train_y).astype('float32')\n",
    "    y_test = np.asarray(test_y).astype('float32')\n",
    "\n",
    "    return x_train, x_test, y_train, y_test, selected_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learning_ml():\n",
    "    \"\"\"학습\"\"\"\n",
    "\n",
    "    x_train, x_test, y_train, y_test, selected_tokens = data_preprocessing('comments2')\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Dense(64, activation='relu', input_shape=(100,)))\n",
    "    model.add(layers.Dense(64, activation='relu'))\n",
    "    model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "    model.compile(optimizer=optimizers.RMSprop(lr=0.001),\n",
    "                  loss=losses.binary_crossentropy,\n",
    "                  metrics=[metrics.binary_accuracy])\n",
    "\n",
    "    model.fit(x_train, y_train, epochs=20, batch_size=512)\n",
    "\n",
    "    results = model.evaluate(x_test, y_test)\n",
    "\n",
    "\n",
    "    model_json = model.to_json()\n",
    "    with open(\"./model.json\", \"w\") as json_file: #학습된 모델저장\n",
    "        json_file.write(model_json)\n",
    "    model.save_weights(\"./model.h5\") #가중치저장\n",
    "\n",
    "    return model\n",
    "\n",
    "learning_ml()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
